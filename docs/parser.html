<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>keboola.json_to_csv.parser API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>keboola.json_to_csv.parser</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import hashlib
from csv import DictWriter
from typing import Any, Dict, List, Optional, Union

from .analyzer import Analyzer
from .csv_row import CsvRow
from .exceptions import JsonParserException
from .mapping import TableMapping
from .node import NodeType
from .table import Table
from .utils import is_dict, is_list, is_scalar


class Parser:
    &#34;&#34;&#34;
    JSON to CSV Parser.

    This class is responsible for converting JSON data to CSV files based on a specified table mapping.
    It provides methods for parsing JSON data, analyzing it to generate table mappings, and saving the
    parsed CSV data to files.

    &#34;&#34;&#34;

    def __init__(self,
                 main_table_name: Optional[str] = None,
                 table_mapping: Optional[TableMapping] = None,
                 analyze_further: bool = True) -&gt; None:
        &#34;&#34;&#34;
        Initialize the JSON Parser.

        Parameters:
            main_table_name (Optional[str]): The name of the main table to use for the CSV files.
            table_mapping (Optional[TableMapping]): The mapping of tables and their columns, primary keys,
                                                    and force types.
            analyze_further (bool): If True, performs further analysis of data that is not specified
                                    in the table mapping.
        &#34;&#34;&#34;
        self.analyze_further = analyze_further
        self.main_table_name = main_table_name
        self.table_mapping = table_mapping

        self._csv_file_results = {}
        self.analyzer = Analyzer(root_name=main_table_name, table_mapping=table_mapping)

    def parse_data_to_csv(self,
                          input_data: Union[Dict, List[Dict]],
                          destination_path: str,
                          root_name: Optional[str] = None) -&gt; None:
        &#34;&#34;&#34;
        Parse input data to CSV files and save them to the specified destination path.

        Parameters:
            input_data (Union[Dict, List[Dict]]): The JSON data to parse.
            destination_path (str): The path to save the generated CSV files.
            root_name (Optional[str]): The root node name to use for parsing the data (default: None).
        &#34;&#34;&#34;
        data_to_parse = self._get_parseable_data_from_input_data(input_data, root_name)
        parsed_data = self._parse_data(data_to_parse)
        for table_id, table in parsed_data.items():
            with open(f&#34;{destination_path}/{table.name}.csv&#34;, &#34;w&#34;) as outfile:
                writer = DictWriter(outfile, table.headers)
                writer.writeheader()
                writer.writerows(table.rows)

    def parse_data(self, input_data: Union[Dict, List[Dict]], root_name: Optional[str] = None) -&gt; Dict:
        &#34;&#34;&#34;
        Parse input data and return a dictionary of CSV rows as flat dictionaries.

        Parameters:
            input_data (Union[Dict, List[Dict]]): The JSON data to parse.
            root_name (Optional[str]): The root node name to use for parsing the data (default: None).

        Returns:
            Dict: A dictionary containing the parsed CSV data.
        &#34;&#34;&#34;
        data_to_parse = self._get_parseable_data_from_input_data(input_data, root_name)
        self._parse_data(data_to_parse)
        return {key: self._csv_file_results[key].rows for key in self._csv_file_results}

    @staticmethod
    def _get_parseable_data_from_input_data(input_data: Union[Dict, List[Dict]],
                                            root_name: Optional[str]) -&gt; List[Dict]:
        &#34;&#34;&#34;
            Get the parseable data from the input JSON data based on the root name.

            This static method processes the input JSON data to obtain a list of dictionaries that can be parsed
            by the parser. It handles cases where a specific root node is provided as the entry point for parsing.

            Parameters:
                input_data (Union[Dict, List[Dict]]): The JSON data to process.
                root_name (Optional[str]): The root node name to use for parsing the data (default: None).

            Returns:
                List[Dict]: A list of dictionaries that can be parsed by the parser.

            Raises:
                JsonParserException: If the specified root node is invalid and not found in the input data.
            &#34;&#34;&#34;
        if root_name:
            if root_name == &#34;.&#34;:
                input_data = [input_data]
            else:
                for node in root_name.split(&#34;.&#34;):
                    if node not in input_data:
                        raise JsonParserException(f&#34;The root node &#39;{root_name}&#39; is invalid&#34;)
                    input_data = input_data.get(node)
        if is_dict(input_data):
            input_data = [input_data]
        return input_data

    def _parse_data(self, input_data: List[Dict],
                    node_path: Optional[List[str]] = None,
                    file_name: Optional[str] = None,
                    parent_data: Optional[Dict[str, str]] = None) -&gt; Dict[str, Table]:
        &#34;&#34;&#34;
        Parse the input JSON data recursively and generate Table objects.

        This private method is responsible for recursively parsing the input JSON data and converting it
        into Table objects.
        The parsed CSV data is stored in the `_csv_file_results` dictionary as Table objects.

        Parameters:
            input_data (List[Dict]): The JSON data to parse.
            node_path (Optional[List[str]]): The path to the current node being analyzed (default: None).
            file_name (Optional[str]): The name of the CSV file (table) to associate the parsed data (default: None).
            parent_data (Optional[Dict[str, str]]): A dictionary containing parent-level data to be merged
            with the current row.

        Returns:
            Dict[str, Table]: A dictionary containing Table objects representing the parsed CSV data.

        Notes:
            - If `file_name` is not provided, the main table name specified during initialization will be used.
            - The `node_path` parameter is used for internal tracking during the recursive parsing process.
            - The `parent_data` parameter allows for merging parent-level data into the current row.

        &#34;&#34;&#34;
        if not file_name:
            file_name = self.main_table_name
        if not node_path:
            node_path = []
        csv_file = self._create_csv_file(file_name)
        if not parent_data:
            parent_data = {}
        for row in input_data:
            if is_scalar(row):
                row = {&#34;data&#34;: row}
            if parent_data:
                row.update(parent_data)
                for key in parent_data:
                    new_node_path = node_path + [key]
                    self.analyzer.add_node(new_node_path, NodeType.SCALAR)
            csv_row = self._parse_row(row, node_path)
            csv_file.save_row(csv_row.get_row())
            csv_file.headers = csv_row.get_headers()

        return self._csv_file_results

    def _parse_row(self,
                   row: Dict[str, Any],
                   current_path: Optional[List[str]] = None,
                   outer_object_hash: Optional[str] = None) -&gt; CsvRow:
        current_path = current_path or []
        if self.analyze_further:
            for name, value in row.items():
                self.analyzer.analyze_object(current_path, name, value)
        columns = self.analyzer.get_column_types(current_path)
        array_parent_id = self._generate_column_hash(row, current_path, outer_object_hash)
        primary_key_values = self._get_primary_key_values(data_row=row, node_path=current_path)
        column_map = self.analyzer.get_column_mappings_at_path(current_path)
        csv_row = CsvRow(column_map)
        for column, (data_type, force_type) in dict(columns).items():
            self._parse_field_by_type(row, csv_row, str(column), data_type, current_path, array_parent_id,
                                      primary_key_values, force_type)
        return csv_row

    def _parse_field_by_type(self,
                             row: Dict[str, Any],
                             csv_row: CsvRow,
                             column: str,
                             data_type: NodeType,
                             node_path: List[str],
                             array_parent_id: str,
                             primary_key_values: Dict[str, str],
                             force_type: bool) -&gt; None:
        &#34;&#34;&#34;
            Parse a field of JSON data based on its data type and other properties.

            This private method is responsible for parsing a field of JSON data based on its data type, node path,
            and other properties like primary keys and force type.
            It then stores the parsed value in the corresponding CSV row.

            Parameters:
                row (Dict[str, Any]): The JSON data for the current row.
                csv_row (CsvRow): The CsvRow object representing the current row in the CSV.
                column (str): The column name of the current field in the JSON data.
                data_type (NodeType): The data type of the current field based on the node structure.
                node_path (List[str]): The path to the current field in the JSON data.
                array_parent_id (str): The ID of the parent object (for array elements) used for CSV table relations.
                primary_key_values (Dict[str, str]): A dictionary containing primary key values for the table.
                force_type (bool): A flag indicating whether to force the data to be unparsed.

            Notes:
                - If the column data is None, it tries to fetch the default value from the node in the analyzer.
                - For lists and dictionaries, it calls the corresponding private methods to handle them recursively.
                - For scalars or when force type is True, it sets the value directly in the CSV row.
                - The `whole_path` variable is used for setting the value in the CSV row with the correct key
                  (dot-separated).

            &#34;&#34;&#34;

        column_data = row.get(column)
        if column_data is None:
            path = node_path + [column]
            node = self.analyzer.get_node_dict(path)
            column_data = node.get(&#34;node&#34;).default_value

        if column_data is None:
            pass
        elif data_type in [NodeType.LIST, NodeType.LIST_OF_DICTS, NodeType.LIST_OF_SCALARS] and not force_type:
            self._parse_list_field(column_data, csv_row, node_path, column, array_parent_id, primary_key_values)
        elif data_type == NodeType.DICT and not force_type:
            # add primary_key_values to parse dict field?
            self._parse_dict_field(column_data, csv_row, node_path, column, array_parent_id)
        elif is_scalar(column_data) or force_type:
            whole_path = node_path + [column]
            csv_row.set_value(&#34;.&#34;.join(whole_path), column_data)

    def _parse_list_field(self,
                          column_data: Any,
                          csv_row: CsvRow,
                          node_path: List[str],
                          column: str,
                          array_parent_id: str,
                          primary_key_values: Dict[str, str]) -&gt; None:
        &#34;&#34;&#34;

        Parse a list field of JSON data.

        This private method is responsible for parsing a list field of JSON data, handling array elements, and
        recursively processing the list items. It creates separate tables for each nested list,
        maintains the parent-child relationship in the CSV data, and applies primary key values if available.

        Parameters:
            column_data (Any): The JSON data for the current column (list field).
            csv_row (CsvRow): The CsvRow object representing the current row in the CSV.
            node_path (List[str]): The path to the current field in the JSON data.
            column (str): The column name of the current field in the JSON data.
            array_parent_id (str): The ID of the parent object (for array elements) used for CSV table relations.
            primary_key_values (Dict[str, str]): A dictionary containing primary key values for the table.

        &#34;&#34;&#34;
        if not is_list(column_data):
            column_data = [column_data]
        if array_parent_id or primary_key_values:
            new_path = self.analyzer.create_path_to_child_object(node_path, column)
            new_table_name = self.analyzer.get_table_name(new_path)
            parent_data = {&#34;JSON_parentId&#34;: array_parent_id}
            if primary_key_values:
                parent_data = primary_key_values
            else:
                child_path = self.analyzer.create_path_to_child_object(node_path, column)
                sf = self.analyzer.get_node_dict(child_path).get(&#34;node&#34;).header_name
                csv_row.set_value(sf, array_parent_id)

            self._parse_data(column_data, new_path, new_table_name, parent_data=parent_data)

    def _parse_dict_field(self,
                          column_data: Dict,
                          csv_row: CsvRow,
                          node_path: List[Union[Any, str]],
                          column: str,
                          array_parent_id: str) -&gt; None:

        new_path = self.analyzer.create_path_to_child_object(node_path, column)
        child_row = self._parse_row(column_data, new_path, outer_object_hash=array_parent_id)
        for key, value in child_row.get_row().items():
            csv_row.set_value(key, value)

    def _create_csv_file(self, file_name: str) -&gt; Table:
        if file_name not in self._csv_file_results:
            self._csv_file_results[file_name] = Table(file_name, [])
        return self._csv_file_results[file_name]

    def _generate_column_hash(self, data_row: Dict[str, Any], node_path: List[Union[Any, str]],
                              outer_object_hash: Optional[str] = None) -&gt; str:
        node_path = [self.main_table_name] + node_path
        clean_node_string = &#34;.&#34;.join(list(filter(lambda val: val != &#34;[]&#34;, node_path)))
        hashed_values = hashlib.md5(str(data_row).encode() + str(outer_object_hash).encode()).hexdigest()
        return f&#34;{clean_node_string}_{hashed_values}&#34;

    def _get_primary_key_values(self, data_row: Dict[str, Any], node_path: List[Union[Any, str]]) -&gt; Dict[str, str]:
        current_table_primary_keys = {}
        children_nodes = self.analyzer.get_node_dict(path_to_object=node_path).get(&#34;children&#34;)
        for child_node in children_nodes:
            if children_nodes.get(child_node).get(&#34;node&#34;).is_primary_key:
                # TODO FIX WHEN ID IS NESTED e.g. some_dict.id
                name = &#34;_&#34;.join([self.main_table_name] + node_path + [child_node])
                current_table_primary_keys[name] = data_row.get(child_node)

        return current_table_primary_keys</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="keboola.json_to_csv.parser.Parser"><code class="flex name class">
<span>class <span class="ident">Parser</span></span>
<span>(</span><span>main_table_name: Optional[str] = None, table_mapping: Optional[<a title="keboola.json_to_csv.mapping.TableMapping" href="mapping.html#keboola.json_to_csv.mapping.TableMapping">TableMapping</a>] = None, analyze_further: bool = True)</span>
</code></dt>
<dd>
<div class="desc"><p>JSON to CSV Parser.</p>
<p>This class is responsible for converting JSON data to CSV files based on a specified table mapping.
It provides methods for parsing JSON data, analyzing it to generate table mappings, and saving the
parsed CSV data to files.</p>
<p>Initialize the JSON Parser.</p>
<h2 id="parameters">Parameters</h2>
<p>main_table_name (Optional[str]): The name of the main table to use for the CSV files.
table_mapping (Optional[TableMapping]): The mapping of tables and their columns, primary keys,
and force types.
analyze_further (bool): If True, performs further analysis of data that is not specified
in the table mapping.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Parser:
    &#34;&#34;&#34;
    JSON to CSV Parser.

    This class is responsible for converting JSON data to CSV files based on a specified table mapping.
    It provides methods for parsing JSON data, analyzing it to generate table mappings, and saving the
    parsed CSV data to files.

    &#34;&#34;&#34;

    def __init__(self,
                 main_table_name: Optional[str] = None,
                 table_mapping: Optional[TableMapping] = None,
                 analyze_further: bool = True) -&gt; None:
        &#34;&#34;&#34;
        Initialize the JSON Parser.

        Parameters:
            main_table_name (Optional[str]): The name of the main table to use for the CSV files.
            table_mapping (Optional[TableMapping]): The mapping of tables and their columns, primary keys,
                                                    and force types.
            analyze_further (bool): If True, performs further analysis of data that is not specified
                                    in the table mapping.
        &#34;&#34;&#34;
        self.analyze_further = analyze_further
        self.main_table_name = main_table_name
        self.table_mapping = table_mapping

        self._csv_file_results = {}
        self.analyzer = Analyzer(root_name=main_table_name, table_mapping=table_mapping)

    def parse_data_to_csv(self,
                          input_data: Union[Dict, List[Dict]],
                          destination_path: str,
                          root_name: Optional[str] = None) -&gt; None:
        &#34;&#34;&#34;
        Parse input data to CSV files and save them to the specified destination path.

        Parameters:
            input_data (Union[Dict, List[Dict]]): The JSON data to parse.
            destination_path (str): The path to save the generated CSV files.
            root_name (Optional[str]): The root node name to use for parsing the data (default: None).
        &#34;&#34;&#34;
        data_to_parse = self._get_parseable_data_from_input_data(input_data, root_name)
        parsed_data = self._parse_data(data_to_parse)
        for table_id, table in parsed_data.items():
            with open(f&#34;{destination_path}/{table.name}.csv&#34;, &#34;w&#34;) as outfile:
                writer = DictWriter(outfile, table.headers)
                writer.writeheader()
                writer.writerows(table.rows)

    def parse_data(self, input_data: Union[Dict, List[Dict]], root_name: Optional[str] = None) -&gt; Dict:
        &#34;&#34;&#34;
        Parse input data and return a dictionary of CSV rows as flat dictionaries.

        Parameters:
            input_data (Union[Dict, List[Dict]]): The JSON data to parse.
            root_name (Optional[str]): The root node name to use for parsing the data (default: None).

        Returns:
            Dict: A dictionary containing the parsed CSV data.
        &#34;&#34;&#34;
        data_to_parse = self._get_parseable_data_from_input_data(input_data, root_name)
        self._parse_data(data_to_parse)
        return {key: self._csv_file_results[key].rows for key in self._csv_file_results}

    @staticmethod
    def _get_parseable_data_from_input_data(input_data: Union[Dict, List[Dict]],
                                            root_name: Optional[str]) -&gt; List[Dict]:
        &#34;&#34;&#34;
            Get the parseable data from the input JSON data based on the root name.

            This static method processes the input JSON data to obtain a list of dictionaries that can be parsed
            by the parser. It handles cases where a specific root node is provided as the entry point for parsing.

            Parameters:
                input_data (Union[Dict, List[Dict]]): The JSON data to process.
                root_name (Optional[str]): The root node name to use for parsing the data (default: None).

            Returns:
                List[Dict]: A list of dictionaries that can be parsed by the parser.

            Raises:
                JsonParserException: If the specified root node is invalid and not found in the input data.
            &#34;&#34;&#34;
        if root_name:
            if root_name == &#34;.&#34;:
                input_data = [input_data]
            else:
                for node in root_name.split(&#34;.&#34;):
                    if node not in input_data:
                        raise JsonParserException(f&#34;The root node &#39;{root_name}&#39; is invalid&#34;)
                    input_data = input_data.get(node)
        if is_dict(input_data):
            input_data = [input_data]
        return input_data

    def _parse_data(self, input_data: List[Dict],
                    node_path: Optional[List[str]] = None,
                    file_name: Optional[str] = None,
                    parent_data: Optional[Dict[str, str]] = None) -&gt; Dict[str, Table]:
        &#34;&#34;&#34;
        Parse the input JSON data recursively and generate Table objects.

        This private method is responsible for recursively parsing the input JSON data and converting it
        into Table objects.
        The parsed CSV data is stored in the `_csv_file_results` dictionary as Table objects.

        Parameters:
            input_data (List[Dict]): The JSON data to parse.
            node_path (Optional[List[str]]): The path to the current node being analyzed (default: None).
            file_name (Optional[str]): The name of the CSV file (table) to associate the parsed data (default: None).
            parent_data (Optional[Dict[str, str]]): A dictionary containing parent-level data to be merged
            with the current row.

        Returns:
            Dict[str, Table]: A dictionary containing Table objects representing the parsed CSV data.

        Notes:
            - If `file_name` is not provided, the main table name specified during initialization will be used.
            - The `node_path` parameter is used for internal tracking during the recursive parsing process.
            - The `parent_data` parameter allows for merging parent-level data into the current row.

        &#34;&#34;&#34;
        if not file_name:
            file_name = self.main_table_name
        if not node_path:
            node_path = []
        csv_file = self._create_csv_file(file_name)
        if not parent_data:
            parent_data = {}
        for row in input_data:
            if is_scalar(row):
                row = {&#34;data&#34;: row}
            if parent_data:
                row.update(parent_data)
                for key in parent_data:
                    new_node_path = node_path + [key]
                    self.analyzer.add_node(new_node_path, NodeType.SCALAR)
            csv_row = self._parse_row(row, node_path)
            csv_file.save_row(csv_row.get_row())
            csv_file.headers = csv_row.get_headers()

        return self._csv_file_results

    def _parse_row(self,
                   row: Dict[str, Any],
                   current_path: Optional[List[str]] = None,
                   outer_object_hash: Optional[str] = None) -&gt; CsvRow:
        current_path = current_path or []
        if self.analyze_further:
            for name, value in row.items():
                self.analyzer.analyze_object(current_path, name, value)
        columns = self.analyzer.get_column_types(current_path)
        array_parent_id = self._generate_column_hash(row, current_path, outer_object_hash)
        primary_key_values = self._get_primary_key_values(data_row=row, node_path=current_path)
        column_map = self.analyzer.get_column_mappings_at_path(current_path)
        csv_row = CsvRow(column_map)
        for column, (data_type, force_type) in dict(columns).items():
            self._parse_field_by_type(row, csv_row, str(column), data_type, current_path, array_parent_id,
                                      primary_key_values, force_type)
        return csv_row

    def _parse_field_by_type(self,
                             row: Dict[str, Any],
                             csv_row: CsvRow,
                             column: str,
                             data_type: NodeType,
                             node_path: List[str],
                             array_parent_id: str,
                             primary_key_values: Dict[str, str],
                             force_type: bool) -&gt; None:
        &#34;&#34;&#34;
            Parse a field of JSON data based on its data type and other properties.

            This private method is responsible for parsing a field of JSON data based on its data type, node path,
            and other properties like primary keys and force type.
            It then stores the parsed value in the corresponding CSV row.

            Parameters:
                row (Dict[str, Any]): The JSON data for the current row.
                csv_row (CsvRow): The CsvRow object representing the current row in the CSV.
                column (str): The column name of the current field in the JSON data.
                data_type (NodeType): The data type of the current field based on the node structure.
                node_path (List[str]): The path to the current field in the JSON data.
                array_parent_id (str): The ID of the parent object (for array elements) used for CSV table relations.
                primary_key_values (Dict[str, str]): A dictionary containing primary key values for the table.
                force_type (bool): A flag indicating whether to force the data to be unparsed.

            Notes:
                - If the column data is None, it tries to fetch the default value from the node in the analyzer.
                - For lists and dictionaries, it calls the corresponding private methods to handle them recursively.
                - For scalars or when force type is True, it sets the value directly in the CSV row.
                - The `whole_path` variable is used for setting the value in the CSV row with the correct key
                  (dot-separated).

            &#34;&#34;&#34;

        column_data = row.get(column)
        if column_data is None:
            path = node_path + [column]
            node = self.analyzer.get_node_dict(path)
            column_data = node.get(&#34;node&#34;).default_value

        if column_data is None:
            pass
        elif data_type in [NodeType.LIST, NodeType.LIST_OF_DICTS, NodeType.LIST_OF_SCALARS] and not force_type:
            self._parse_list_field(column_data, csv_row, node_path, column, array_parent_id, primary_key_values)
        elif data_type == NodeType.DICT and not force_type:
            # add primary_key_values to parse dict field?
            self._parse_dict_field(column_data, csv_row, node_path, column, array_parent_id)
        elif is_scalar(column_data) or force_type:
            whole_path = node_path + [column]
            csv_row.set_value(&#34;.&#34;.join(whole_path), column_data)

    def _parse_list_field(self,
                          column_data: Any,
                          csv_row: CsvRow,
                          node_path: List[str],
                          column: str,
                          array_parent_id: str,
                          primary_key_values: Dict[str, str]) -&gt; None:
        &#34;&#34;&#34;

        Parse a list field of JSON data.

        This private method is responsible for parsing a list field of JSON data, handling array elements, and
        recursively processing the list items. It creates separate tables for each nested list,
        maintains the parent-child relationship in the CSV data, and applies primary key values if available.

        Parameters:
            column_data (Any): The JSON data for the current column (list field).
            csv_row (CsvRow): The CsvRow object representing the current row in the CSV.
            node_path (List[str]): The path to the current field in the JSON data.
            column (str): The column name of the current field in the JSON data.
            array_parent_id (str): The ID of the parent object (for array elements) used for CSV table relations.
            primary_key_values (Dict[str, str]): A dictionary containing primary key values for the table.

        &#34;&#34;&#34;
        if not is_list(column_data):
            column_data = [column_data]
        if array_parent_id or primary_key_values:
            new_path = self.analyzer.create_path_to_child_object(node_path, column)
            new_table_name = self.analyzer.get_table_name(new_path)
            parent_data = {&#34;JSON_parentId&#34;: array_parent_id}
            if primary_key_values:
                parent_data = primary_key_values
            else:
                child_path = self.analyzer.create_path_to_child_object(node_path, column)
                sf = self.analyzer.get_node_dict(child_path).get(&#34;node&#34;).header_name
                csv_row.set_value(sf, array_parent_id)

            self._parse_data(column_data, new_path, new_table_name, parent_data=parent_data)

    def _parse_dict_field(self,
                          column_data: Dict,
                          csv_row: CsvRow,
                          node_path: List[Union[Any, str]],
                          column: str,
                          array_parent_id: str) -&gt; None:

        new_path = self.analyzer.create_path_to_child_object(node_path, column)
        child_row = self._parse_row(column_data, new_path, outer_object_hash=array_parent_id)
        for key, value in child_row.get_row().items():
            csv_row.set_value(key, value)

    def _create_csv_file(self, file_name: str) -&gt; Table:
        if file_name not in self._csv_file_results:
            self._csv_file_results[file_name] = Table(file_name, [])
        return self._csv_file_results[file_name]

    def _generate_column_hash(self, data_row: Dict[str, Any], node_path: List[Union[Any, str]],
                              outer_object_hash: Optional[str] = None) -&gt; str:
        node_path = [self.main_table_name] + node_path
        clean_node_string = &#34;.&#34;.join(list(filter(lambda val: val != &#34;[]&#34;, node_path)))
        hashed_values = hashlib.md5(str(data_row).encode() + str(outer_object_hash).encode()).hexdigest()
        return f&#34;{clean_node_string}_{hashed_values}&#34;

    def _get_primary_key_values(self, data_row: Dict[str, Any], node_path: List[Union[Any, str]]) -&gt; Dict[str, str]:
        current_table_primary_keys = {}
        children_nodes = self.analyzer.get_node_dict(path_to_object=node_path).get(&#34;children&#34;)
        for child_node in children_nodes:
            if children_nodes.get(child_node).get(&#34;node&#34;).is_primary_key:
                # TODO FIX WHEN ID IS NESTED e.g. some_dict.id
                name = &#34;_&#34;.join([self.main_table_name] + node_path + [child_node])
                current_table_primary_keys[name] = data_row.get(child_node)

        return current_table_primary_keys</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="keboola.json_to_csv.parser.Parser.parse_data"><code class="name flex">
<span>def <span class="ident">parse_data</span></span>(<span>self, input_data: Union[Dict, List[Dict]], root_name: Optional[str] = None) ‑> Dict</span>
</code></dt>
<dd>
<div class="desc"><p>Parse input data and return a dictionary of CSV rows as flat dictionaries.</p>
<h2 id="parameters">Parameters</h2>
<p>input_data (Union[Dict, List[Dict]]): The JSON data to parse.
root_name (Optional[str]): The root node name to use for parsing the data (default: None).</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Dict</code></dt>
<dd>A dictionary containing the parsed CSV data.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def parse_data(self, input_data: Union[Dict, List[Dict]], root_name: Optional[str] = None) -&gt; Dict:
    &#34;&#34;&#34;
    Parse input data and return a dictionary of CSV rows as flat dictionaries.

    Parameters:
        input_data (Union[Dict, List[Dict]]): The JSON data to parse.
        root_name (Optional[str]): The root node name to use for parsing the data (default: None).

    Returns:
        Dict: A dictionary containing the parsed CSV data.
    &#34;&#34;&#34;
    data_to_parse = self._get_parseable_data_from_input_data(input_data, root_name)
    self._parse_data(data_to_parse)
    return {key: self._csv_file_results[key].rows for key in self._csv_file_results}</code></pre>
</details>
</dd>
<dt id="keboola.json_to_csv.parser.Parser.parse_data_to_csv"><code class="name flex">
<span>def <span class="ident">parse_data_to_csv</span></span>(<span>self, input_data: Union[Dict, List[Dict]], destination_path: str, root_name: Optional[str] = None) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Parse input data to CSV files and save them to the specified destination path.</p>
<h2 id="parameters">Parameters</h2>
<p>input_data (Union[Dict, List[Dict]]): The JSON data to parse.
destination_path (str): The path to save the generated CSV files.
root_name (Optional[str]): The root node name to use for parsing the data (default: None).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def parse_data_to_csv(self,
                      input_data: Union[Dict, List[Dict]],
                      destination_path: str,
                      root_name: Optional[str] = None) -&gt; None:
    &#34;&#34;&#34;
    Parse input data to CSV files and save them to the specified destination path.

    Parameters:
        input_data (Union[Dict, List[Dict]]): The JSON data to parse.
        destination_path (str): The path to save the generated CSV files.
        root_name (Optional[str]): The root node name to use for parsing the data (default: None).
    &#34;&#34;&#34;
    data_to_parse = self._get_parseable_data_from_input_data(input_data, root_name)
    parsed_data = self._parse_data(data_to_parse)
    for table_id, table in parsed_data.items():
        with open(f&#34;{destination_path}/{table.name}.csv&#34;, &#34;w&#34;) as outfile:
            writer = DictWriter(outfile, table.headers)
            writer.writeheader()
            writer.writerows(table.rows)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="keboola.json_to_csv" href="index.html">keboola.json_to_csv</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="keboola.json_to_csv.parser.Parser" href="#keboola.json_to_csv.parser.Parser">Parser</a></code></h4>
<ul class="">
<li><code><a title="keboola.json_to_csv.parser.Parser.parse_data" href="#keboola.json_to_csv.parser.Parser.parse_data">parse_data</a></code></li>
<li><code><a title="keboola.json_to_csv.parser.Parser.parse_data_to_csv" href="#keboola.json_to_csv.parser.Parser.parse_data_to_csv">parse_data_to_csv</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>